# logdir = os.path.join(output, task_name)
output: ./output_dir/
task_name: base_exp

dataset:
  train:
    source_list: 'datasets/GTA/train.txt'
    target_list: 'datasets/Cityscapes/train.txt'
    width: 512
    height: 256
    transform: ['h_flip', 'v_flip'] #['h_flip', 'v_flip', 'crop', 'normalize']
    random_pair: True
    return_name: False
    batch_size: 1
  test:
    source_list: 'datasets/GTA/test.txt'
    target_list: 'datasets/Cityscapes/test.txt'
    width: 512
    height: 256
    transform: [] #['h_flip', 'v_flip', 'crop', 'normalize']
    random_pair: False
    return_name: True
    batch_size: 16

lr: 0.0001
max_iter: 300000
print_freq: 100
save_freq: 500
resume: False
load_path: ''

network:
  configurable: False #[True, False]
  pad_size: 10
  in_channel: 3
  out_channels: [30, 120]
  weight_type: 'learned' #['fixed', 'sigmoid', 'softmax', 'attention', 'learned']

loss:
  vgg_encoder: 'model/losses/vgg_model/vgg_normalised.pth'
  k: 0.8
  weight: 0.1

lr_scheduler:
  type: cosine
  eta_min: 0.0000000
